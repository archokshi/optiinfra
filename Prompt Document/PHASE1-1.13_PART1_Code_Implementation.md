# PHASE1-1.13 PART1: Comprehensive Unit Tests - Code Implementation

**Phase:** Cost Agent - Week 2  
**Objective:** Create comprehensive unit tests for all Cost Agent components  
**Priority:** HIGH  
**Estimated Effort:** 2-3 hours  
**Date:** October 23, 2025

---

## ðŸ“‹ OVERVIEW

PHASE1-1.13 focuses on **creating comprehensive unit tests** for all Cost Agent components. After implementing all core functionality (Data Collection, Analysis, LLM Integration, Recommendations, Execution, Learning Loop, and API Endpoints), we now need to ensure everything is thoroughly tested.

### What We're Testing
1. **Data Collection** - AWS, GCP, Azure cost collectors
2. **Cost Analysis** - Anomaly detection, trend analysis, forecasting
3. **LLM Integration** - Groq API integration, prompt engineering
4. **Recommendations** - Recommendation generation and validation
5. **Execution** - Execution engine and rollback mechanisms
6. **Learning Loop** - Outcome tracking and model improvement
7. **API Endpoints** - All REST API endpoints
8. **Utilities** - Helper functions and shared utilities

**Key Differences from Previous Phases:**
- **PHASE1-1.1-1.11:** Built core functionality
- **PHASE1-1.12:** Added API endpoints and authentication
- **PHASE1-1.13:** **Comprehensive unit testing for ALL components**

**Expected Impact:** 95%+ code coverage, production-ready testing suite, confidence in deployments

---

## ðŸŽ¯ OBJECTIVES

### Primary Goals
1. **Comprehensive Coverage:**
   - Test all core components
   - Cover edge cases
   - Test error handling
   - Validate business logic

2. **Test Organization:**
   - Structured test suite
   - Clear test naming
   - Reusable fixtures
   - Mock external dependencies

3. **Quality Assurance:**
   - 95%+ code coverage
   - Fast test execution
   - Reliable tests (no flakiness)
   - Clear failure messages

4. **CI/CD Integration:**
   - Automated test execution
   - Coverage reporting
   - Test result tracking
   - Performance benchmarks

### Success Criteria
- âœ… 95%+ code coverage
- âœ… All tests passing
- âœ… < 60 seconds total test execution time
- âœ… 0 flaky tests
- âœ… All edge cases covered
- âœ… Mock all external dependencies

---

## ðŸ—ï¸ TEST ARCHITECTURE

### Test Organization

```
tests/
â”œâ”€â”€ unit/                           # Unit tests
â”‚   â”œâ”€â”€ collectors/                 # Data collection tests
â”‚   â”‚   â”œâ”€â”€ test_aws_collector.py
â”‚   â”‚   â”œâ”€â”€ test_gcp_collector.py
â”‚   â”‚   â”œâ”€â”€ test_azure_collector.py
â”‚   â”‚   â””â”€â”€ test_vultr_collector.py
â”‚   â”œâ”€â”€ analysis/                   # Analysis tests
â”‚   â”‚   â”œâ”€â”€ test_anomaly_detector.py
â”‚   â”‚   â”œâ”€â”€ test_trend_analyzer.py
â”‚   â”‚   â””â”€â”€ test_forecaster.py
â”‚   â”œâ”€â”€ llm/                        # LLM integration tests
â”‚   â”‚   â”œâ”€â”€ test_groq_client.py
â”‚   â”‚   â”œâ”€â”€ test_prompt_manager.py
â”‚   â”‚   â””â”€â”€ test_llm_analyzer.py
â”‚   â”œâ”€â”€ recommendations/            # Recommendation tests
â”‚   â”‚   â”œâ”€â”€ test_generator.py
â”‚   â”‚   â”œâ”€â”€ test_validator.py
â”‚   â”‚   â””â”€â”€ test_prioritizer.py
â”‚   â”œâ”€â”€ execution/                  # Execution tests
â”‚   â”‚   â”œâ”€â”€ test_executor.py
â”‚   â”‚   â”œâ”€â”€ test_rollback.py
â”‚   â”‚   â””â”€â”€ test_state_machine.py
â”‚   â”œâ”€â”€ learning/                   # Learning loop tests
â”‚   â”‚   â”œâ”€â”€ test_outcome_tracker.py
â”‚   â”‚   â”œâ”€â”€ test_metrics.py
â”‚   â”‚   â””â”€â”€ test_insights.py
â”‚   â””â”€â”€ utils/                      # Utility tests
â”‚       â”œâ”€â”€ test_helpers.py
â”‚       â””â”€â”€ test_validators.py
â”œâ”€â”€ integration/                    # Integration tests
â”‚   â”œâ”€â”€ test_end_to_end.py
â”‚   â””â”€â”€ test_workflows.py
â”œâ”€â”€ fixtures/                       # Test fixtures
â”‚   â”œâ”€â”€ cost_data.py
â”‚   â”œâ”€â”€ recommendations.py
â”‚   â””â”€â”€ mock_responses.py
â””â”€â”€ conftest.py                     # Pytest configuration
```

---

## ðŸ“¦ IMPLEMENTATION PHASES

### Phase 1: Test Infrastructure Setup (20 min)

**Objective:** Set up testing infrastructure and fixtures

**Tasks:**
1. Configure pytest
2. Create test fixtures
3. Set up mocking utilities
4. Create test data generators

**Files to Create:**
- `tests/conftest.py` - Pytest configuration
- `tests/fixtures/cost_data.py` - Cost data fixtures
- `tests/fixtures/recommendations.py` - Recommendation fixtures
- `tests/fixtures/mock_responses.py` - Mock API responses
- `pytest.ini` - Pytest settings

---

### Phase 2: Data Collection Tests (25 min)

**Objective:** Test all cost collectors

**Components to Test:**
1. **AWS Collector**
   - Cost retrieval
   - Data parsing
   - Error handling
   - Pagination

2. **GCP Collector**
   - BigQuery integration
   - Cost aggregation
   - Service breakdown

3. **Azure Collector**
   - Cost Management API
   - Resource grouping
   - Tag filtering

4. **Vultr Collector**
   - API integration
   - Cost calculation
   - Resource mapping

**Files to Create:**
- `tests/unit/collectors/test_aws_collector.py`
- `tests/unit/collectors/test_gcp_collector.py`
- `tests/unit/collectors/test_azure_collector.py`
- `tests/unit/collectors/test_vultr_collector.py`

---

### Phase 3: Analysis Tests (25 min)

**Objective:** Test cost analysis components

**Components to Test:**
1. **Anomaly Detection**
   - Statistical analysis
   - Threshold detection
   - Severity classification
   - False positive handling

2. **Trend Analysis**
   - Time series analysis
   - Pattern recognition
   - Seasonality detection

3. **Forecasting**
   - Prediction accuracy
   - Confidence intervals
   - Model validation

**Files to Create:**
- `tests/unit/analysis/test_anomaly_detector.py`
- `tests/unit/analysis/test_trend_analyzer.py`
- `tests/unit/analysis/test_forecaster.py`

---

### Phase 4: LLM Integration Tests (20 min)

**Objective:** Test LLM integration

**Components to Test:**
1. **Groq Client**
   - API calls
   - Response parsing
   - Error handling
   - Retry logic

2. **Prompt Manager**
   - Prompt generation
   - Context injection
   - Token counting

3. **LLM Analyzer**
   - Analysis requests
   - Response validation
   - Fallback handling

**Files to Create:**
- `tests/unit/llm/test_groq_client.py`
- `tests/unit/llm/test_prompt_manager.py`
- `tests/unit/llm/test_llm_analyzer.py`

---

### Phase 5: Recommendation Tests (25 min)

**Objective:** Test recommendation engine

**Components to Test:**
1. **Generator**
   - Recommendation creation
   - Savings calculation
   - Risk assessment
   - Resource selection

2. **Validator**
   - Recommendation validation
   - Constraint checking
   - Feasibility analysis

3. **Prioritizer**
   - Priority scoring
   - Ranking logic
   - Filter application

**Files to Create:**
- `tests/unit/recommendations/test_generator.py`
- `tests/unit/recommendations/test_validator.py`
- `tests/unit/recommendations/test_prioritizer.py`

---

### Phase 6: Execution Tests (25 min)

**Objective:** Test execution engine

**Components to Test:**
1. **Executor**
   - Execution logic
   - State transitions
   - Error handling
   - Dry run mode

2. **Rollback**
   - Rollback logic
   - State restoration
   - Cleanup operations

3. **State Machine**
   - State transitions
   - Event handling
   - Validation rules

**Files to Create:**
- `tests/unit/execution/test_executor.py`
- `tests/unit/execution/test_rollback.py`
- `tests/unit/execution/test_state_machine.py`

---

### Phase 7: Learning Loop Tests (20 min)

**Objective:** Test learning loop components

**Components to Test:**
1. **Outcome Tracker**
   - Outcome recording
   - Metrics calculation
   - Data aggregation

2. **Metrics**
   - Accuracy metrics
   - Performance metrics
   - Trend metrics

3. **Insights**
   - Pattern detection
   - Insight generation
   - Confidence scoring

**Files to Create:**
- `tests/unit/learning/test_outcome_tracker.py`
- `tests/unit/learning/test_metrics.py`
- `tests/unit/learning/test_insights.py`

---

### Phase 8: Integration Tests (20 min)

**Objective:** Test end-to-end workflows

**Scenarios to Test:**
1. **Full Recommendation Flow**
   - Data collection â†’ Analysis â†’ Recommendation â†’ Execution
   
2. **Error Recovery**
   - Failure scenarios
   - Rollback flows
   - Retry logic

3. **Learning Loop**
   - Outcome tracking
   - Model improvement
   - Feedback integration

**Files to Create:**
- `tests/integration/test_end_to_end.py`
- `tests/integration/test_workflows.py`

---

## ðŸ“‹ DETAILED IMPLEMENTATION

### Phase 1: Test Infrastructure Setup

#### Step 1.1: Pytest Configuration

**File:** `pytest.ini`

```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v
    --strict-markers
    --tb=short
    --cov=src
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=95
markers =
    unit: Unit tests
    integration: Integration tests
    slow: Slow running tests
    aws: AWS-specific tests
    gcp: GCP-specific tests
    azure: Azure-specific tests
asyncio_mode = auto
```

#### Step 1.2: Test Fixtures

**File:** `tests/conftest.py`

```python
"""
Pytest Configuration and Shared Fixtures.
"""

import pytest
from datetime import datetime, timedelta
from typing import Dict, List
import json


@pytest.fixture
def sample_aws_costs():
    """Sample AWS cost data."""
    return {
        "customer_id": "cust-test",
        "provider": "aws",
        "start_date": "2025-10-01",
        "end_date": "2025-10-23",
        "total_cost": 15420.50,
        "services": [
            {"service": "EC2", "cost": 8500.00},
            {"service": "RDS", "cost": 4200.00},
            {"service": "S3", "cost": 2720.50}
        ]
    }


@pytest.fixture
def sample_recommendation():
    """Sample recommendation data."""
    return {
        "id": "rec-test-123",
        "customer_id": "cust-test",
        "type": "spot_migration",
        "title": "Migrate to Spot Instances",
        "description": "Migrate 10 EC2 instances to spot",
        "estimated_monthly_savings": 1200.00,
        "priority": "high",
        "risk_level": "low",
        "affected_resources": ["i-123", "i-456"]
    }


@pytest.fixture
def mock_groq_response():
    """Mock Groq API response."""
    return {
        "choices": [{
            "message": {
                "content": json.dumps({
                    "analysis": "Cost spike detected in EC2",
                    "recommendations": ["Consider spot instances"],
                    "confidence": 0.95
                })
            }
        }]
    }


@pytest.fixture
def sample_execution():
    """Sample execution data."""
    return {
        "id": "exec-test-123",
        "recommendation_id": "rec-test-123",
        "customer_id": "cust-test",
        "status": "completed",
        "started_at": datetime.utcnow(),
        "completed_at": datetime.utcnow() + timedelta(minutes=5),
        "success": True
    }
```

**File:** `tests/fixtures/cost_data.py`

```python
"""
Cost Data Fixtures.
"""

from datetime import datetime, timedelta
from typing import List, Dict


def generate_daily_costs(days: int = 30, base_cost: float = 500.0) -> List[Dict]:
    """Generate daily cost data for testing."""
    costs = []
    start_date = datetime.utcnow() - timedelta(days=days)
    
    for i in range(days):
        date = start_date + timedelta(days=i)
        # Add some variation
        daily_cost = base_cost + (i * 10) + ((-1) ** i * 50)
        
        costs.append({
            "date": date.strftime("%Y-%m-%d"),
            "cost": round(daily_cost, 2),
            "services": {
                "EC2": round(daily_cost * 0.55, 2),
                "RDS": round(daily_cost * 0.27, 2),
                "S3": round(daily_cost * 0.18, 2)
            }
        })
    
    return costs


def generate_anomaly_data(normal_cost: float = 500.0, spike_multiplier: float = 2.0) -> List[Dict]:
    """Generate cost data with anomalies."""
    costs = generate_daily_costs(30, normal_cost)
    
    # Add anomalies on specific days
    anomaly_days = [10, 20, 25]
    for day in anomaly_days:
        costs[day]["cost"] *= spike_multiplier
        costs[day]["anomaly"] = True
    
    return costs
```

---

### Phase 2: Data Collection Tests

#### Step 2.1: AWS Collector Tests

**File:** `tests/unit/collectors/test_aws_collector.py`

```python
"""
Unit Tests for AWS Cost Collector.
"""

import pytest
from unittest.mock import Mock, patch, MagicMock
from datetime import datetime, timedelta
from src.collectors.aws_collector import AWSCostCollector


class TestAWSCostCollector:
    """Test AWS cost collector."""
    
    @pytest.fixture
    def collector(self):
        """Create AWS collector instance."""
        return AWSCostCollector(
            access_key="test_key",
            secret_key="test_secret",
            region="us-east-1"
        )
    
    @pytest.fixture
    def mock_boto_client(self):
        """Mock boto3 client."""
        with patch('boto3.client') as mock:
            yield mock
    
    def test_collector_initialization(self, collector):
        """Test collector initialization."""
        assert collector.region == "us-east-1"
        assert collector.access_key == "test_key"
    
    @pytest.mark.asyncio
    async def test_get_costs_success(self, collector, mock_boto_client):
        """Test successful cost retrieval."""
        # Mock response
        mock_client = MagicMock()
        mock_client.get_cost_and_usage.return_value = {
            "ResultsByTime": [{
                "TimePeriod": {
                    "Start": "2025-10-01",
                    "End": "2025-10-02"
                },
                "Total": {
                    "UnblendedCost": {
                        "Amount": "500.00",
                        "Unit": "USD"
                    }
                }
            }]
        }
        mock_boto_client.return_value = mock_client
        
        # Test
        result = await collector.get_costs(
            customer_id="cust-test",
            start_date=datetime(2025, 10, 1),
            end_date=datetime(2025, 10, 2)
        )
        
        assert result["total_cost"] == 500.00
        assert result["provider"] == "aws"
    
    @pytest.mark.asyncio
    async def test_get_costs_with_service_breakdown(self, collector, mock_boto_client):
        """Test cost retrieval with service breakdown."""
        # Mock response with services
        mock_client = MagicMock()
        mock_client.get_cost_and_usage.return_value = {
            "ResultsByTime": [{
                "Groups": [
                    {
                        "Keys": ["EC2"],
                        "Metrics": {"UnblendedCost": {"Amount": "300.00"}}
                    },
                    {
                        "Keys": ["RDS"],
                        "Metrics": {"UnblendedCost": {"Amount": "200.00"}}
                    }
                ]
            }]
        }
        mock_boto_client.return_value = mock_client
        
        result = await collector.get_costs_by_service(
            customer_id="cust-test",
            start_date=datetime(2025, 10, 1),
            end_date=datetime(2025, 10, 2)
        )
        
        assert len(result["services"]) == 2
        assert result["services"][0]["service"] == "EC2"
        assert result["services"][0]["cost"] == 300.00
    
    @pytest.mark.asyncio
    async def test_get_costs_error_handling(self, collector, mock_boto_client):
        """Test error handling in cost retrieval."""
        # Mock error
        mock_client = MagicMock()
        mock_client.get_cost_and_usage.side_effect = Exception("API Error")
        mock_boto_client.return_value = mock_client
        
        with pytest.raises(Exception) as exc_info:
            await collector.get_costs(
                customer_id="cust-test",
                start_date=datetime(2025, 10, 1),
                end_date=datetime(2025, 10, 2)
            )
        
        assert "API Error" in str(exc_info.value)
    
    def test_parse_cost_response(self, collector):
        """Test cost response parsing."""
        raw_response = {
            "ResultsByTime": [{
                "Total": {
                    "UnblendedCost": {
                        "Amount": "1234.56",
                        "Unit": "USD"
                    }
                }
            }]
        }
        
        result = collector._parse_cost_response(raw_response)
        assert result == 1234.56
    
    def test_validate_date_range(self, collector):
        """Test date range validation."""
        start = datetime(2025, 10, 1)
        end = datetime(2025, 10, 31)
        
        assert collector._validate_date_range(start, end) == True
        
        # Test invalid range
        with pytest.raises(ValueError):
            collector._validate_date_range(end, start)
```

---

### Phase 3: Analysis Tests

#### Step 3.1: Anomaly Detector Tests

**File:** `tests/unit/analysis/test_anomaly_detector.py`

```python
"""
Unit Tests for Anomaly Detector.
"""

import pytest
from src.analysis.anomaly_detector import AnomalyDetector
from tests.fixtures.cost_data import generate_anomaly_data


class TestAnomalyDetector:
    """Test anomaly detection."""
    
    @pytest.fixture
    def detector(self):
        """Create anomaly detector."""
        return AnomalyDetector(
            sensitivity=2.0,  # 2 standard deviations
            min_data_points=7
        )
    
    def test_detector_initialization(self, detector):
        """Test detector initialization."""
        assert detector.sensitivity == 2.0
        assert detector.min_data_points == 7
    
    def test_detect_anomalies_with_spike(self, detector):
        """Test anomaly detection with cost spike."""
        # Generate data with anomalies
        cost_data = generate_anomaly_data(normal_cost=500.0, spike_multiplier=3.0)
        
        anomalies = detector.detect(cost_data)
        
        assert len(anomalies) > 0
        assert all(a["severity"] in ["low", "medium", "high"] for a in anomalies)
    
    def test_detect_no_anomalies(self, detector):
        """Test detection with no anomalies."""
        # Generate normal data
        from tests.fixtures.cost_data import generate_daily_costs
        cost_data = generate_daily_costs(30, 500.0)
        
        anomalies = detector.detect(cost_data)
        
        assert len(anomalies) == 0
    
    def test_calculate_threshold(self, detector):
        """Test threshold calculation."""
        values = [100, 110, 105, 108, 102, 115, 109]
        
        threshold = detector._calculate_threshold(values)
        
        assert threshold > 0
        assert isinstance(threshold, float)
    
    def test_classify_severity(self, detector):
        """Test anomaly severity classification."""
        # Small deviation
        severity = detector._classify_severity(deviation=1.5)
        assert severity == "low"
        
        # Medium deviation
        severity = detector._classify_severity(deviation=2.5)
        assert severity == "medium"
        
        # Large deviation
        severity = detector._classify_severity(deviation=4.0)
        assert severity == "high"
```

---

## ðŸ“Š TEST COVERAGE TARGETS

### Component Coverage Goals

| Component | Target Coverage | Priority |
|-----------|----------------|----------|
| **Data Collectors** | 95% | HIGH |
| **Analysis Engine** | 95% | HIGH |
| **LLM Integration** | 90% | MEDIUM |
| **Recommendations** | 95% | HIGH |
| **Execution Engine** | 98% | CRITICAL |
| **Learning Loop** | 90% | MEDIUM |
| **API Endpoints** | 95% | HIGH |
| **Utilities** | 90% | MEDIUM |

### Overall Target: 95%+ Code Coverage

---

## âœ… ACCEPTANCE CRITERIA

### Must Have
- âœ… 95%+ code coverage
- âœ… All tests passing
- âœ… < 60 seconds total execution time
- âœ… 0 flaky tests
- âœ… All edge cases covered
- âœ… Mock all external dependencies

### Should Have
- âœ… Integration tests for critical flows
- âœ… Performance benchmarks
- âœ… Test documentation
- âœ… CI/CD integration ready

### Nice to Have
- âœ… Property-based testing
- âœ… Mutation testing
- âœ… Load testing
- âœ… Chaos testing

---

## ðŸš€ NEXT STEPS

After completing PART1:
1. Execute PART2 (Execution and Validation)
2. Run all tests and verify coverage
3. Fix any failing tests
4. Generate coverage report
5. Document test results
6. Integrate with CI/CD

---

**END OF PART1 SPECIFICATION**
