# PHASE1-1.6c PART1: Right-Sizing Workflow - Code Implementation

**Phase:** Cost Agent - Week 2  
**Objective:** Implement production-ready Right-Sizing Optimization Workflow  
**Base:** Extends PHASE1-1.6 (Spot Migration) and PHASE1-1.6b (RI Optimization)  
**Date:** October 22, 2025

---

## ðŸ“‹ OVERVIEW

The Right-Sizing Workflow analyzes instance utilization patterns to identify over-provisioned or under-provisioned resources and recommends optimal instance types to reduce costs while maintaining performance.

**Key Differences from Previous Workflows:**
- **PHASE1-1.6 (Spot):** Pricing model optimization (on-demand â†’ spot)
- **PHASE1-1.6b (RI):** Commitment-based savings (on-demand â†’ reserved)
- **PHASE1-1.6c (Right-Sizing):** Resource optimization (current size â†’ optimal size)

**Expected Savings:** 20-50% through instance type optimization

---

## ðŸŽ¯ OBJECTIVES

### Primary Goals
1. Analyze historical resource utilization (CPU, memory, network, disk)
2. Identify over-provisioned instances (low utilization)
3. Identify under-provisioned instances (high utilization, throttling)
4. Recommend optimal instance types based on actual usage
5. Calculate cost savings and performance impact
6. Provide migration recommendations with risk assessment

### Success Criteria
- âœ… Accurate utilization analysis (7-90 days historical data)
- âœ… Multi-dimensional analysis (CPU, memory, network, disk, IOPS)
- âœ… Family-aware recommendations (stay in same family or suggest alternatives)
- âœ… Cost-performance trade-off analysis
- âœ… Risk assessment (performance degradation risk)
- âœ… Integration with ClickHouse and Prometheus
- âœ… 85%+ test coverage

---

## ðŸ—ï¸ ARCHITECTURE

### Workflow Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Right-Sizing Workflow                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. collect_metrics_data                                     â”‚
â”‚     â””â”€> Gather CPU, memory, network, disk metrics           â”‚
â”‚                                                              â”‚
â”‚  2. analyze_utilization                                      â”‚
â”‚     â”œâ”€> Calculate utilization percentiles (p50, p95, p99)   â”‚
â”‚     â”œâ”€> Detect over/under-provisioning                      â”‚
â”‚     â””â”€> Identify optimization candidates                    â”‚
â”‚                                                              â”‚
â”‚  3. generate_rightsizing_recommendations                     â”‚
â”‚     â”œâ”€> Match optimal instance types                        â”‚
â”‚     â”œâ”€> Calculate cost savings                              â”‚
â”‚     â””â”€> Assess performance risk                             â”‚
â”‚                                                              â”‚
â”‚  4. calculate_impact_analysis                                â”‚
â”‚     â”œâ”€> Cost impact (savings vs investment)                 â”‚
â”‚     â”œâ”€> Performance impact (risk assessment)                â”‚
â”‚     â””â”€> Migration complexity                                â”‚
â”‚                                                              â”‚
â”‚  5. coordinate_approval (future)                             â”‚
â”‚     â””â”€> Customer approval workflow                          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
Cloud Metrics â†’ Utilization Analysis â†’ Recommendation Engine â†’ Impact Analysis â†’ Response
     â†“                    â†“                      â†“                    â†“
ClickHouse          Prometheus            ClickHouse          Prometheus
```

---

## ðŸ“¦ IMPLEMENTATION PLAN

### Phase 1: Core Analysis Module
**File:** `src/nodes/rightsizing_analyze.py`

**Functions:**
1. `analyze_utilization_patterns(state) -> Dict`
   - Analyze CPU, memory, network, disk utilization
   - Calculate percentiles (p50, p95, p99, max)
   - Detect over-provisioning (low utilization)
   - Detect under-provisioning (high utilization, throttling)
   - Identify optimization candidates

2. `calculate_resource_metrics(instance) -> Dict`
   - CPU utilization metrics
   - Memory utilization metrics
   - Network throughput metrics
   - Disk IOPS and throughput metrics
   - Burstable credit metrics (for T-family instances)

3. `detect_provisioning_issue(metrics) -> str`
   - Returns: "over_provisioned", "under_provisioned", "optimal", "unknown"
   - Logic:
     - Over: p95 CPU < 40% AND p95 Memory < 50%
     - Under: p95 CPU > 80% OR p95 Memory > 85% OR throttling detected
     - Optimal: Within acceptable ranges

4. `calculate_optimization_score(metrics) -> float`
   - Score 0.0-1.0 indicating optimization potential
   - Factors: utilization gap, cost savings potential, stability

**Key Metrics:**
- CPU: p50, p95, p99, max, average
- Memory: p50, p95, p99, max, average
- Network: p95 throughput (in/out)
- Disk: p95 IOPS (read/write), p95 throughput
- Burstable: credit balance, credit usage rate

---

### Phase 2: Recommendation Engine
**File:** `src/nodes/rightsizing_recommend.py`

**Functions:**
1. `generate_rightsizing_recommendations(state) -> Dict`
   - For each optimization candidate:
     - Find optimal instance type
     - Calculate cost savings
     - Assess performance risk
     - Generate migration plan

2. `find_optimal_instance_type(current_instance, metrics, preferences) -> Dict`
   - Match based on actual resource needs
   - Consider instance families:
     - Same family (e.g., t3.large â†’ t3.medium)
     - Alternative families (e.g., t3 â†’ t4g for ARM)
   - Filter by:
     - Region availability
     - Required vCPUs
     - Required memory
     - Network performance
     - Storage requirements

3. `calculate_rightsizing_savings(current, recommended) -> Dict`
   - Cost comparison (hourly, monthly, annual)
   - Savings percentage
   - Break-even analysis (if migration cost exists)
   - ROI calculation

4. `assess_performance_risk(current_metrics, recommended_instance) -> str`
   - Returns: "low", "medium", "high"
   - Factors:
     - Resource headroom in recommended instance
     - Peak utilization patterns
     - Burstable vs non-burstable
     - Network performance requirements

5. `generate_migration_plan(current, recommended) -> Dict`
   - Migration steps
   - Downtime estimate
   - Rollback plan
   - Testing recommendations

**Instance Type Matching Logic:**
```python
# Priority order for recommendations:
1. Same family, smaller size (e.g., t3.xlarge â†’ t3.large)
2. Same generation, different family (e.g., m5.large â†’ t3.xlarge)
3. Newer generation, same family (e.g., t3.large â†’ t4g.large)
4. Alternative architecture (e.g., x86 â†’ ARM/Graviton)
```

---

### Phase 3: Impact Analysis
**File:** `src/nodes/rightsizing_impact.py`

**Functions:**
1. `calculate_impact_analysis(state) -> Dict`
   - Aggregate all recommendations
   - Calculate total cost impact
   - Assess overall performance risk
   - Generate executive summary

2. `calculate_cost_impact(recommendations) -> Dict`
   - Total current cost
   - Total recommended cost
   - Total savings (monthly, annual)
   - Average savings percentage
   - Breakdown by service type

3. `calculate_performance_impact(recommendations) -> Dict`
   - Risk distribution (low/medium/high)
   - Instances requiring testing
   - Instances safe to migrate
   - Performance improvement opportunities

4. `calculate_migration_complexity(recommendations) -> Dict`
   - Simple migrations (same family, no downtime)
   - Complex migrations (family change, downtime required)
   - High-risk migrations (performance concerns)
   - Estimated total migration time

5. `generate_impact_summary(recommendations) -> Dict`
   - Executive summary
   - Quick wins (high savings, low risk)
   - Recommendations by priority
   - Implementation roadmap

---

### Phase 4: Pydantic Models
**File:** `src/models/rightsizing_optimization.py`

**Models:**

1. **RightSizingRequest**
```python
class RightSizingRequest(BaseModel):
    customer_id: str  # Regex: ^[a-zA-Z0-9_-]{1,64}$
    cloud_provider: str  # Enum: aws, gcp, azure
    service_types: List[str] = ["ec2"]  # ec2, rds, elasticache
    analysis_period_days: int = 30  # 7-90 days
    min_utilization_threshold: float = 40.0  # 0-100%
    max_utilization_threshold: float = 80.0  # 0-100%
    include_burstable: bool = True
    include_arm: bool = True  # Include ARM/Graviton instances
    customer_preferences: Optional[Dict[str, Any]] = None
```

2. **ResourceMetrics**
```python
class ResourceMetrics(BaseModel):
    cpu_p50: float
    cpu_p95: float
    cpu_p99: float
    cpu_max: float
    memory_p50: float
    memory_p95: float
    memory_p99: float
    memory_max: float
    network_in_p95: float  # Mbps
    network_out_p95: float  # Mbps
    disk_read_iops_p95: Optional[float] = None
    disk_write_iops_p95: Optional[float] = None
    throttling_events: int = 0
    burstable_credit_balance: Optional[float] = None
```

3. **RightSizingRecommendation**
```python
class RightSizingRecommendation(BaseModel):
    instance_id: str
    current_instance_type: str
    recommended_instance_type: str
    service_type: str
    region: str
    
    # Current metrics
    current_metrics: ResourceMetrics
    
    # Cost analysis
    current_hourly_cost: float
    recommended_hourly_cost: float
    hourly_savings: float
    monthly_savings: float
    annual_savings: float
    savings_percent: float
    
    # Capacity comparison
    current_vcpus: int
    recommended_vcpus: int
    current_memory_gb: float
    recommended_memory_gb: float
    
    # Risk assessment
    performance_risk: str  # low, medium, high
    risk_factors: List[str]
    confidence_score: float  # 0.0-1.0
    
    # Migration
    migration_complexity: str  # simple, moderate, complex
    estimated_downtime_minutes: int
    requires_testing: bool
    
    # Metadata
    optimization_type: str  # downsize, upsize, family_change
    provisioning_issue: str  # over_provisioned, under_provisioned
```

4. **ImpactAnalysis**
```python
class ImpactAnalysis(BaseModel):
    total_instances_analyzed: int
    optimization_candidates: int
    
    # Cost impact
    total_current_monthly_cost: float
    total_recommended_monthly_cost: float
    total_monthly_savings: float
    total_annual_savings: float
    average_savings_percent: float
    
    # Performance impact
    low_risk_count: int
    medium_risk_count: int
    high_risk_count: int
    
    # Migration complexity
    simple_migrations: int
    moderate_migrations: int
    complex_migrations: int
    
    # Breakdown
    downsize_count: int
    upsize_count: int
    family_change_count: int
    
    # Summary
    quick_wins: List[str]  # Instance IDs with high savings, low risk
    requires_testing: List[str]  # Instance IDs requiring testing
    implementation_priority: List[Dict[str, Any]]
```

5. **RightSizingResponse**
```python
class RightSizingResponse(BaseModel):
    request_id: str
    customer_id: str
    cloud_provider: str
    timestamp: datetime
    
    # Analysis results
    instances_analyzed: int
    optimization_candidates: int
    recommendations: List[RightSizingRecommendation]
    
    # Financial summary
    total_monthly_savings: float
    total_annual_savings: float
    average_savings_percent: float
    
    # Impact analysis
    impact_analysis: ImpactAnalysis
    
    # Workflow status
    workflow_status: str
    success: bool
    error_message: Optional[str] = None
```

---

### Phase 5: LangGraph Workflow
**File:** `src/workflows/rightsizing_optimization.py`

**Workflow State:**
```python
class RightSizingWorkflowState(TypedDict):
    request_id: str
    customer_id: str
    cloud_provider: str
    service_types: List[str]
    analysis_period_days: int
    
    # Collected data
    instance_metrics: List[Dict[str, Any]]
    
    # Analysis results
    optimization_candidates: List[Dict[str, Any]]
    
    # Recommendations
    rightsizing_recommendations: List[Dict[str, Any]]
    
    # Impact analysis
    impact_analysis: Dict[str, Any]
    
    # Summary
    total_monthly_savings: float
    total_annual_savings: float
    
    # Status
    workflow_status: str
    error_message: Optional[str]
```

**Workflow Class:**
```python
class ProductionRightSizingWorkflow:
    def __init__(
        self,
        aws_credentials: Optional[Dict] = None,
        gcp_credentials: Optional[Dict] = None,
        azure_credentials: Optional[Dict] = None
    ):
        # Initialize cloud collectors
        # Initialize metrics clients
        # Create workflow graph
    
    async def collect_metrics_data(
        self,
        customer_id: str,
        cloud_provider: str,
        days: int
    ) -> List[Dict]:
        """Collect detailed resource metrics from cloud provider"""
        
    async def run_optimization(
        self,
        customer_id: str,
        cloud_provider: str,
        service_types: List[str] = ["ec2"],
        analysis_period_days: int = 30,
        **kwargs
    ) -> Dict:
        """Run complete right-sizing optimization workflow"""
```

---

### Phase 6: ClickHouse Metrics Enhancement
**File:** `src/database/clickhouse_metrics.py`

**New Table:**
```sql
CREATE TABLE IF NOT EXISTS rightsizing_optimization_events (
    timestamp DateTime,
    request_id String,
    customer_id String,
    cloud_provider String,
    service_type String,
    workflow_phase String,
    
    -- Analysis metrics
    instances_analyzed UInt32,
    optimization_candidates UInt32,
    over_provisioned_count UInt32,
    under_provisioned_count UInt32,
    
    -- Recommendation metrics
    recommendations_generated UInt32,
    downsize_count UInt32,
    upsize_count UInt32,
    family_change_count UInt32,
    
    -- Cost metrics
    total_current_cost Float64,
    total_recommended_cost Float64,
    monthly_savings Float64,
    annual_savings Float64,
    average_savings_percent Float32,
    
    -- Risk metrics
    low_risk_count UInt32,
    medium_risk_count UInt32,
    high_risk_count UInt32,
    
    -- Migration metrics
    simple_migrations UInt32,
    moderate_migrations UInt32,
    complex_migrations UInt32,
    
    -- Status
    success UInt8,
    error_message String,
    duration_ms UInt32
) ENGINE = MergeTree()
ORDER BY (timestamp, customer_id)
TTL timestamp + INTERVAL 90 DAY
```

**New Methods:**
- `insert_rightsizing_optimization_event(event: Dict) -> None`
- `get_customer_rightsizing_savings(customer_id: str, days: int) -> Dict`
- `get_rightsizing_trends(customer_id: str, days: int) -> Dict`

---

### Phase 7: Prometheus Metrics Enhancement
**File:** `src/monitoring/prometheus_metrics.py`

**New Metrics:**

**Counters:**
```python
rightsizing_optimizations_total = Counter(
    'rightsizing_optimizations_total',
    'Total right-sizing optimizations',
    ['customer_id', 'cloud_provider', 'status']
)

rightsizing_recommendations_total = Counter(
    'rightsizing_recommendations_total',
    'Total right-sizing recommendations',
    ['customer_id', 'optimization_type', 'risk_level']
)
```

**Histograms:**
```python
rightsizing_savings_percent = Histogram(
    'rightsizing_savings_percent',
    'Right-sizing savings percentage distribution',
    ['customer_id', 'optimization_type'],
    buckets=[10, 20, 30, 40, 50, 60, 70, 80]
)

rightsizing_utilization_gap = Histogram(
    'rightsizing_utilization_gap',
    'Utilization gap (over/under-provisioning)',
    ['customer_id', 'resource_type'],
    buckets=[10, 20, 30, 40, 50, 60, 70, 80, 90]
)
```

**Gauges:**
```python
rightsizing_optimization_candidates = Gauge(
    'rightsizing_optimization_candidates',
    'Number of instances eligible for right-sizing',
    ['customer_id', 'provisioning_issue']
)

rightsizing_average_cpu_utilization = Gauge(
    'rightsizing_average_cpu_utilization',
    'Average CPU utilization across instances',
    ['customer_id', 'instance_family']
)
```

**New Functions:**
- `record_rightsizing_optimization_start()`
- `record_rightsizing_optimization_complete()`
- `record_rightsizing_recommendation()`
- `update_utilization_metrics()`

---

## ðŸ”’ SECURITY & VALIDATION

### Input Validation
```python
# Customer ID validation
customer_id: str = Field(
    ...,
    pattern=r'^[a-zA-Z0-9_-]{1,64}$',
    description="Customer identifier"
)

# Utilization thresholds
min_utilization_threshold: float = Field(
    default=40.0,
    ge=0.0,
    le=100.0,
    description="Minimum utilization threshold"
)

max_utilization_threshold: float = Field(
    default=80.0,
    ge=0.0,
    le=100.0,
    description="Maximum utilization threshold"
)

# Analysis period
analysis_period_days: int = Field(
    default=30,
    ge=7,
    le=90,
    description="Analysis period in days"
)
```

### Error Handling
- Retry logic with exponential backoff (tenacity)
- Graceful degradation if metrics unavailable
- Specific exception types
- No sensitive data in logs
- Structured error messages

---

## ðŸ§ª TESTING STRATEGY

### Test File
**File:** `tests/test_rightsizing_production.py`

### Test Categories

1. **Utilization Analysis Tests** (6 tests)
   - test_identify_over_provisioned_instances
   - test_identify_under_provisioned_instances
   - test_calculate_resource_metrics
   - test_detect_provisioning_issue
   - test_calculate_optimization_score
   - test_handle_missing_metrics

2. **Recommendation Tests** (6 tests)
   - test_generate_rightsizing_recommendations
   - test_find_optimal_instance_type_downsize
   - test_find_optimal_instance_type_upsize
   - test_calculate_rightsizing_savings
   - test_assess_performance_risk
   - test_generate_migration_plan

3. **Impact Analysis Tests** (4 tests)
   - test_calculate_impact_analysis
   - test_calculate_cost_impact
   - test_calculate_performance_impact
   - test_generate_impact_summary

4. **Workflow Tests** (3 tests)
   - test_workflow_initialization
   - test_collect_metrics_data
   - test_complete_workflow

5. **Metrics Tests** (3 tests)
   - test_clickhouse_insert_event
   - test_get_customer_savings
   - test_prometheus_metrics_recording

6. **Validation Tests** (8 tests)
   - test_valid_request
   - test_invalid_thresholds
   - test_invalid_analysis_period
   - test_recommendation_validation
   - test_metrics_validation
   - test_impact_analysis_validation
   - test_edge_cases
   - test_error_handling

**Target:** 30+ tests, 85%+ coverage

---

## ðŸ“Š SUCCESS METRICS

### Technical Metrics
- âœ… 85%+ test coverage
- âœ… All tests passing
- âœ… API response time < 15s
- âœ… Error rate < 1%
- âœ… Accurate utilization analysis

### Business Metrics
- âœ… Identify 30%+ of instances as optimization candidates
- âœ… Average savings 20-50% per recommendation
- âœ… 80%+ recommendations are low-medium risk
- âœ… Clear migration plans for all recommendations

### Monitoring Metrics
- âœ… Optimization requests tracked
- âœ… Recommendations per customer tracked
- âœ… Savings per recommendation tracked
- âœ… Risk distribution tracked
- âœ… Processing duration tracked

---

## ðŸ”„ INTEGRATION POINTS

### With Existing Workflows
- **PHASE1-1.6 (Spot):** Can combine spot + right-sizing for maximum savings
- **PHASE1-1.6b (RI):** Right-size first, then purchase RIs for optimal instances
- **Cloud Collectors:** Reuse existing AWS/GCP/Azure collectors
- **Metrics Infrastructure:** Reuse ClickHouse and Prometheus setup

### Data Sources
- CloudWatch (AWS) - CPU, memory, network, disk metrics
- Cloud Monitoring (GCP) - Resource utilization
- Azure Monitor - Performance metrics
- Custom metrics from agents (if available)

---

## ðŸ“ IMPLEMENTATION STEPS

### Step 1: Create Analysis Module (30 min)
- Implement `rightsizing_analyze.py`
- Add utilization analysis functions
- Add provisioning detection logic
- Add error handling

### Step 2: Create Recommendation Engine (30 min)
- Implement `rightsizing_recommend.py`
- Add instance type matching logic
- Add savings calculations
- Add risk assessment

### Step 3: Create Impact Analysis (20 min)
- Implement `rightsizing_impact.py`
- Add cost impact calculations
- Add performance impact analysis
- Add migration complexity assessment

### Step 4: Create Pydantic Models (20 min)
- Implement `rightsizing_optimization.py`
- Add all validation models
- Add field validators

### Step 5: Create LangGraph Workflow (30 min)
- Implement `rightsizing_optimization.py` (workflow)
- Create workflow graph
- Add metrics integration
- Add error handling

### Step 6: Enhance Metrics (20 min)
- Update `clickhouse_metrics.py`
- Update `prometheus_metrics.py`
- Add new tables and metrics

### Step 7: Create Tests (40 min)
- Implement `test_rightsizing_production.py`
- Create 30+ comprehensive tests
- Achieve 85%+ coverage

### Step 8: Run and Validate (20 min)
- Run all tests
- Fix any failures
- Validate integration

**Total Estimated Time:** ~3.5 hours

---

## ðŸŽ¯ DELIVERABLES

### Code Files
1. âœ… `src/nodes/rightsizing_analyze.py` (~400 lines)
2. âœ… `src/nodes/rightsizing_recommend.py` (~400 lines)
3. âœ… `src/nodes/rightsizing_impact.py` (~300 lines)
4. âœ… `src/models/rightsizing_optimization.py` (~250 lines)
5. âœ… `src/workflows/rightsizing_optimization.py` (~500 lines)
6. âœ… `src/database/clickhouse_metrics.py` (enhanced, +150 lines)
7. âœ… `src/monitoring/prometheus_metrics.py` (enhanced, +250 lines)
8. âœ… `tests/test_rightsizing_production.py` (~700 lines)

### Documentation
1. âœ… PHASE1-1.6c_PART1_Code_Implementation.md (this file)
2. âœ… PHASE1-1.6c_PART2_Execution_and_Validation.md
3. âœ… Implementation summary
4. âœ… Validation summary

**Total New Code:** ~3,000 lines

---

## ðŸš€ DEPLOYMENT CONSIDERATIONS

### Prerequisites
- âœ… Cloud credentials configured
- âœ… CloudWatch/monitoring access
- âœ… ClickHouse server (optional)
- âœ… Prometheus server (optional)

### Configuration
```python
# Example configuration
RIGHTSIZING_CONFIG = {
    "min_utilization_threshold": 40.0,  # Over-provisioned if below
    "max_utilization_threshold": 80.0,  # Under-provisioned if above
    "analysis_period_days": 30,
    "include_burstable": True,
    "include_arm": True,
    "min_savings_threshold": 10.0,  # Minimum savings to recommend
}
```

### Performance Considerations
- Metrics collection can be slow for large fleets
- Cache instance type catalog
- Parallel processing for multiple instances
- Pagination for large result sets

---

## ðŸ“– EXAMPLE USAGE

```python
from src.workflows.rightsizing_optimization import ProductionRightSizingWorkflow

# Initialize workflow
workflow = ProductionRightSizingWorkflow(
    aws_credentials={
        "access_key": "YOUR_ACCESS_KEY",
        "secret_key": "YOUR_SECRET_KEY",
        "region": "us-east-1"
    }
)

# Run optimization
result = await workflow.run_optimization(
    customer_id="customer-123",
    cloud_provider="aws",
    service_types=["ec2"],
    analysis_period_days=30,
    min_utilization_threshold=40.0,
    max_utilization_threshold=80.0,
    include_burstable=True,
    include_arm=True
)

# Results
print(f"Instances Analyzed: {result['instances_analyzed']}")
print(f"Optimization Candidates: {result['optimization_candidates']}")
print(f"Recommendations: {len(result['recommendations'])}")
print(f"Monthly Savings: ${result['total_monthly_savings']:,.2f}")
print(f"Annual Savings: ${result['total_annual_savings']:,.2f}")
print(f"Average Savings: {result['average_savings_percent']:.1f}%")

# Impact analysis
impact = result['impact_analysis']
print(f"\nLow Risk: {impact['low_risk_count']}")
print(f"Medium Risk: {impact['medium_risk_count']}")
print(f"High Risk: {impact['high_risk_count']}")
print(f"\nQuick Wins: {len(impact['quick_wins'])} instances")
```

---

**Document Version:** 1.0  
**Last Updated:** October 22, 2025  
**Next:** PHASE1-1.6c_PART2_Execution_and_Validation.md
