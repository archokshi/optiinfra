# üèóÔ∏è OptiInfra - Complete Architecture Overview

**Version:** 1.1 | **Date:** October 30, 2025 | **Status:** Phase 6.6 In Progress

---

## üéØ Executive Summary

**OptiInfra** is a multi-agent AI platform that automatically optimizes LLM infrastructure:
- **Cut costs by 50%** through intelligent resource optimization
- **Improve performance 3x** via AI-powered tuning  
- **Ensure quality** with continuous monitoring

**Target:** Companies running LLM inference (vLLM, TGI, SGLang)  
**Problem:** $50K-$500K/month waste + 20 hrs/week manual work  
**Solution:** 4 AI agents + orchestrator + unified data collection

---

## üìä High-Level Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              CUSTOMER INFRASTRUCTURE (Multi-Cloud)                ‚îÇ
‚îÇ  AWS vLLM ‚îÇ GCP vLLM ‚îÇ Azure TGI ‚îÇ Vultr SGLang                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ Metrics Collection (Every 15 min)
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    OPTIINFRA PLATFORM                             ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ           DATA COLLECTION LAYER (Port 8005)                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Data Collector (FastAPI + Celery)                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ Cost Collector                                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ Performance Collector                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ Resource Collector                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚îî‚îÄ Application Collector ‚Üí Groq LLM (Quality Analysis)  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                         ‚Üì                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ              DATA STORAGE LAYER                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  PostgreSQL ‚îÇ ClickHouse ‚îÇ Qdrant ‚îÇ Redis                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (Primary)  ‚îÇ(Time-Series)‚îÇ(Vector)‚îÇ(Cache)                ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                         ‚Üì                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ         ORCHESTRATION LAYER (Port 8080)                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Orchestrator (Go) - Routing, Coordination, Conflicts      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                         ‚Üì                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ              AGENT LAYER (AI Brains)                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇCost Agent‚îÇ ‚îÇPerf Agent‚îÇ ‚îÇResource  ‚îÇ ‚îÇApp Agent ‚îÇ      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇPort 8001 ‚îÇ ‚îÇPort 8002 ‚îÇ ‚îÇPort 8003 ‚îÇ ‚îÇPort 8004 ‚îÇ      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ+ Groq LLM‚îÇ ‚îÇ+ Groq LLM‚îÇ ‚îÇ+ Groq LLM‚îÇ ‚îÇ+ Groq LLM‚îÇ      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                         ‚Üì                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ              API LAYER (V1 & V2)                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  V1: Agent-specific ‚îÇ V2: ClickHouse direct (fast)         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           CUSTOMER PORTAL (Next.js 14) - Port 3000               ‚îÇ
‚îÇ  Dashboards ‚îÇ Recommendations ‚îÇ Approvals ‚îÇ History              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîÑ Complete Data Flow

```
STEP 1: COLLECTION (Every 15 min, automated)
Customer Infrastructure ‚Üí Data Collector ‚Üí Groq LLM ‚Üí ClickHouse
                                                    ‚Üì
                                              Redis Pub/Sub

STEP 2: ANALYSIS (On-demand or event-triggered)
Orchestrator ‚Üí Agent ‚Üí ClickHouse/PostgreSQL ‚Üí LangGraph Workflow
                                             ‚Üì
                                        Groq LLM (insights)
                                             ‚Üì
                                    PostgreSQL (recommendations)
                                             ‚Üì
                                    Qdrant (learning context)

STEP 3: COORDINATION (Multi-agent decisions)
Orchestrator ‚Üí Cost Agent: "Migrate to spot?"
            ‚Üí Perf Agent: "Impact on latency?"
            ‚Üí Resource Agent: "Capacity OK?"
            ‚Üí App Agent: "Quality baseline?"
            ‚Üí Resolve conflicts ‚Üí Request approval

STEP 4: APPROVAL (Customer decision)
Portal ‚Üí Customer approves/rejects ‚Üí PostgreSQL ‚Üí Orchestrator

STEP 5: EXECUTION (Gradual rollout)
Agent ‚Üí Execute change (10% ‚Üí 50% ‚Üí 100%)
     ‚Üí App Agent monitors quality
     ‚Üí Auto-rollback if quality drops > 10%
     ‚Üí Record outcome in Qdrant

STEP 6: LEARNING (Continuous improvement)
Qdrant ‚Üí Similar case retrieval ‚Üí Improve future recommendations
```

---

## üß© Component Details

### 1. DATA COLLECTION LAYER

**Component:** Data Collector (FastAPI + Celery)  
**Port:** 8005  
**Technology:** Python 3.11

**Value Proposition:**
- Single source of truth for all metrics
- Multi-cloud support (AWS, GCP, Azure, Vultr)
- Parallel collection (all providers simultaneously)
- Fault tolerance (retry logic, error handling)

**What It Does:**
1. **Cost Collection:** Cloud billing APIs, resource tagging
2. **Performance Collection:** vLLM/TGI/SGLang metrics, Prometheus
3. **Resource Collection:** GPU (nvidia-smi), CPU/Memory (psutil)
4. **Application Collection:** LLM interactions ‚Üí Groq quality analysis

**Key Feature:** Uses Groq LLM to analyze application quality in real-time

---

### 2. DATA STORAGE LAYER

#### **PostgreSQL (Port 5432)**
**Purpose:** Transactional data and business logic

**Value Proposition:**
- ACID compliance for critical data
- Relational integrity for complex queries

**What's Stored:**
- Customers, users, authentication
- Agents, capabilities, health
- Recommendations, approvals, executions
- Credentials (encrypted)
- Learning outcomes

---

#### **ClickHouse (Ports 8123, 9000)**
**Purpose:** High-performance time-series metrics

**Value Proposition:**
- 100x faster than PostgreSQL for time-series
- Columnar storage, 10:1 compression
- Sub-100ms queries for 30 days of data

**What's Stored:**
- Cost metrics (hourly/daily)
- Performance metrics (1-min granularity)
- Resource metrics (1-min granularity)
- Application quality metrics (per interaction)

**Performance:** <100ms for 30-day aggregations

---

#### **Qdrant (Ports 6333, 6334)**
**Purpose:** Vector database for LLM memory

**Value Proposition:**
- Semantic search for similar decisions
- Fast retrieval (<10ms)
- Learning from past outcomes

**What's Stored:**
- Decision embeddings (recommendation context)
- Outcome embeddings (success/failure patterns)
- Historical patterns for improvement

**Use Case:** "Find similar spot migration decisions and their outcomes"

---

#### **Redis (Port 6379)**
**Purpose:** Cache and event messaging

**Value Proposition:**
- Sub-millisecond cache hits
- Real-time pub/sub events
- Celery task queue

**What's Stored:**
- API response cache (1-hour TTL)
- Agent health status (30-sec TTL)
- Pub/sub channels (data_updated, collection_status, etc.)
- Celery task queue

---

### 3. ORCHESTRATION LAYER

**Component:** Orchestrator (Go)  
**Port:** 8080  
**Technology:** Go 1.21, Gin framework

**Value Proposition:**
- High performance (Go concurrency)
- Low resource usage (single binary)
- Intelligent routing (capability-based)
- Conflict resolution (multi-agent)

**Key Responsibilities:**
1. **Agent Registry:** Track agents, monitor health
2. **Request Routing:** Route to appropriate agent(s), load balancing
3. **Multi-Agent Coordination:** Coordinate complex workflows
4. **Conflict Resolution:** Priority: Customer > Perf > Cost
5. **Change Approval:** Manage approval workflow
6. **Event Publishing:** Publish to Redis pub/sub

**Example:** Cost Agent wants spot migration ‚Üí Orchestrator asks Perf/Resource/App agents ‚Üí Resolves conflicts ‚Üí Requests approval

---

### 4. AGENT LAYER (AI Brains)

#### **Cost Agent (Port 8001)**
**Technology:** Python 3.10, FastAPI, LangGraph, Groq LLM

**Value Proposition:** 40-60% cost savings through AI optimization

**Capabilities:**
1. **Spot Migration:** 30-40% savings, risk assessment
2. **Right-Sizing:** 20-30% savings, zero-downtime
3. **Reserved Instances:** 40-60% savings, commitment planning
4. **Idle Detection:** Identify waste, recommend termination

**LLM Use:** Generate business insights, executive summaries, natural language queries

**Goal:** Minimize cloud spending while maintaining performance

---

#### **Performance Agent (Port 8002)**
**Technology:** Python 3.10, FastAPI, LangGraph, Groq LLM

**Value Proposition:** 2-3x performance improvement through AI tuning

**Capabilities:**
1. **Bottleneck Analysis:** Root cause with LLM explanation
2. **KV Cache Optimization:** Tune cache size/eviction
3. **Quantization:** FP16 ‚Üí FP8 ‚Üí INT8 recommendations
4. **Batch Size Optimization:** Throughput vs latency balance
5. **Model Parallelism:** Tensor/pipeline parallelism

**LLM Use:** Explain bottlenecks, suggest optimizations, recommend configs

**Goal:** Maximize throughput and minimize latency

---

#### **Resource Agent (Port 8003)**
**Technology:** Python 3.10, FastAPI, LangGraph, Groq LLM

**Value Proposition:** Maximize GPU/CPU utilization (target: 80%+)

**Capabilities:**
1. **GPU Optimization:** Monitor utilization, consolidation
2. **Auto-Scaling:** Predictive scaling based on patterns
3. **Resource Consolidation:** Identify opportunities, calculate savings
4. **Capacity Planning:** Predict future needs, optimize cost vs perf

**LLM Use:** Analyze utilization patterns, scaling recommendations, consolidation opportunities

**Goal:** Maximize resource efficiency and utilization

---

#### **Application Agent (Port 8004)**
**Technology:** Python 3.10, FastAPI, LangGraph, Groq LLM

**Value Proposition:** Prevent quality regressions before production

**Capabilities:**
1. **Quality Baseline:** Establish benchmarks, track over time
2. **Regression Detection:** Detect drops, auto-rollback if > 10%
3. **A/B Testing:** Compare variants, statistical significance
4. **Approval/Rejection:** Approve if quality maintained, reject if drops

**LLM Use:** Aggregate quality scores, trend analysis, decision making

**Goal:** Ensure LLM output quality remains high during optimizations

---

### 5. API LAYER

#### **V1 APIs (Agent-Specific)**
**Purpose:** Agent-specific operations and workflows

**Endpoints:**
- `/api/v1/cost/recommendations/generate`
- `/api/v1/performance/analyze`
- `/api/v1/resources/optimize`
- `/api/v1/applications/quality`

**Characteristics:**
- Agent-specific logic
- Complex workflows
- Slower (requires agent processing)

---

#### **V2 APIs (ClickHouse Direct)** ‚≠ê Phase 6.5
**Purpose:** Fast data retrieval for dashboards

**Endpoints:**
- `/api/v2/performance/{customer_id}/{provider}/summary`
- `/api/v2/resources/{customer_id}/{provider}/summary`
- `/api/v2/applications/{customer_id}/{provider}/summary`

**Characteristics:**
- Direct ClickHouse queries
- Pre-aggregated data
- Fast (<100ms)
- Read-only

**Goal:** Provide blazing-fast dashboard data without agent overhead

---

### 6. CUSTOMER PORTAL

**Component:** Next.js 14 Dashboard  
**Port:** 3000  
**Technology:** Next.js 14, TypeScript, TailwindCSS, Recharts

**Value Proposition:**
- Real-time visibility into all optimizations
- One-click approvals
- Historical tracking
- Beautiful, modern UI

**Pages:**
1. **Overview:** All agents status, key metrics
2. **Cost Dashboard:** Spending, savings, recommendations
3. **Performance Dashboard:** Latency, throughput, bottlenecks
4. **Resource Dashboard:** Utilization, scaling, capacity
5. **Application Dashboard:** Quality scores, regressions, A/B tests
6. **Recommendations:** Approve/reject with risk assessment
7. **History:** Past executions, outcomes, learnings
8. **Settings:** Credentials, webhooks, notifications

**Goal:** Empower customers to optimize with confidence

---

## üéØ Value Proposition by Component

| Component | Primary Goal | Key Metric | Customer Benefit |
|-----------|--------------|------------|------------------|
| **Data Collector** | Unified metrics | 15-min collection | Single source of truth |
| **PostgreSQL** | Transactional data | ACID compliance | Data integrity |
| **ClickHouse** | Fast analytics | <100ms queries | Real-time insights |
| **Qdrant** | AI learning | <10ms retrieval | Smarter recommendations |
| **Redis** | Speed & events | <1ms cache | Fast responses |
| **Orchestrator** | Coordination | Multi-agent | Unified decisions |
| **Cost Agent** | Save money | 40-60% savings | Lower bills |
| **Performance Agent** | Speed up | 2-3x faster | Better UX |
| **Resource Agent** | Maximize use | 80%+ utilization | Efficiency |
| **Application Agent** | Quality guard | <5% regression | Safe changes |
| **V1 APIs** | Agent logic | Complex workflows | Full features |
| **V2 APIs** | Fast data | <100ms response | Instant dashboards |
| **Portal** | Visibility | Real-time | Control & confidence |

---

## üîë Key Architectural Decisions

### 1. **Why 5 LLM Integration Points?**
- **4 Agents:** Decision-making and analysis
- **1 Data Collector:** Real-time quality measurement
- **Reason:** Separation of concerns (measurement vs analysis)

### 2. **Why ClickHouse + PostgreSQL?**
- **ClickHouse:** Time-series metrics (100x faster)
- **PostgreSQL:** Transactional data (ACID compliance)
- **Reason:** Right tool for the right job

### 3. **Why Go for Orchestrator?**
- **Performance:** Handle thousands of concurrent requests
- **Resource efficiency:** Low memory footprint
- **Concurrency:** Native goroutines for parallel coordination
- **Reason:** Python too slow for coordination layer

### 4. **Why V2 APIs?**
- **V1:** Agent-specific logic (slower, complex)
- **V2:** Direct ClickHouse (faster, simple)
- **Reason:** Dashboards need speed, not complex logic

### 5. **Why Qdrant?**
- **Learning:** Store decision context as vectors
- **Similarity:** Find similar past decisions
- **Improvement:** Learn from outcomes
- **Reason:** Enable AI agents to learn and improve

---

## üìà System Capacity

**Current Configuration (Development):**
- Data Collection: 4 providers √ó 4 data types = 16 collectors
- Collection Frequency: Every 15 minutes = 96 collections/day
- Agents: 4 agents √ó 2 instances each = 8 agent instances
- API Throughput: ~1000 req/sec (orchestrator)
- Storage: 90 days retention in ClickHouse
- Groq API: ~3000-6000 calls/month

**Production Scaling:**
- Horizontal: Add more agent instances
- Vertical: Increase ClickHouse resources
- Geographic: Multi-region deployment
- High Availability: Active-active setup

---

## üåê Phase 6.6: Multi-Cloud Generic Collector

**Status:** üîÑ In Progress  
**Goal:** Universal cloud provider support through Generic Collector pattern

### Overview

Phase 6.6 extends OptiInfra's data collection capabilities to support **15+ cloud providers** using a unified Generic Collector architecture. Instead of creating separate collectors for each provider, we implement a single universal collector that works with any infrastructure exposing Prometheus metrics.

### Architecture Pattern

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CLOUD PROVIDERS (15+)                                      ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Tier 1: Big 3 (Dedicated Collectors)                      ‚îÇ
‚îÇ  ‚îú‚îÄ AWS Collector (native SDK + CloudWatch)                ‚îÇ
‚îÇ  ‚îú‚îÄ GCP Collector (native SDK + Cloud Monitoring)          ‚îÇ
‚îÇ  ‚îî‚îÄ Azure Collector (native SDK + Azure Monitor)           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Tier 2: All Others (Generic Collector)                    ‚îÇ
‚îÇ  ‚îú‚îÄ Vultr, RunPod, DigitalOcean, Linode                   ‚îÇ
‚îÇ  ‚îú‚îÄ Hetzner, OVHcloud, Lambda Labs, CoreWeave             ‚îÇ
‚îÇ  ‚îú‚îÄ Paperspace, On-Premises, Kubernetes, Docker           ‚îÇ
‚îÇ  ‚îî‚îÄ Any infrastructure with Prometheus + optional API      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  GENERIC COLLECTOR (Universal)                              ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Collection Methods:                                        ‚îÇ
‚îÇ  1. Prometheus Scraping (universal, required)              ‚îÇ
‚îÇ  2. DCGM GPU Metrics (universal for NVIDIA)                ‚îÇ
‚îÇ  3. Provider API (optional, for billing/cost data)         ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Provider API Adapters (pluggable):                        ‚îÇ
‚îÇ  ‚îú‚îÄ vultr_api.py      ‚îú‚îÄ runpod_api.py                    ‚îÇ
‚îÇ  ‚îú‚îÄ digitalocean_api.py ‚îú‚îÄ linode_api.py                  ‚îÇ
‚îÇ  ‚îú‚îÄ hetzner_api.py    ‚îú‚îÄ lambda_api.py                    ‚îÇ
‚îÇ  ‚îî‚îÄ ... (easy to add new providers)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Supported Providers (15+)

| Provider | Collection Method | Status | Use Case |
|----------|------------------|--------|----------|
| **AWS** | Dedicated Collector | ‚úÖ Implemented | Enterprise, multi-service |
| **GCP** | Dedicated Collector | ‚úÖ Implemented | Enterprise, multi-service |
| **Azure** | Dedicated Collector | ‚úÖ Implemented | Enterprise, multi-service |
| **Vultr** | Generic + API | üîÑ Phase 6.6 | GPU cloud, demo |
| **RunPod** | Generic + API | üîÑ Phase 6.6 | GPU cloud, training |
| **DigitalOcean** | Generic + API | üîÑ Phase 6.6 | General compute |
| **Linode/Akamai** | Generic + API | üîÑ Phase 6.6 | General compute |
| **Hetzner** | Generic + API | üîÑ Phase 6.6 | EU hosting |
| **OVHcloud** | Generic + API | üîÑ Phase 6.6 | EU hosting |
| **Lambda Labs** | Generic + API | üîÑ Phase 6.6 | GPU cloud |
| **CoreWeave** | Generic + API | üîÑ Phase 6.6 | GPU cloud |
| **Paperspace** | Generic + API | üîÑ Phase 6.6 | GPU cloud |
| **On-Premises** | Generic (Prometheus only) | üîÑ Phase 6.6 | Private datacenter |
| **Kubernetes** | Generic + K8s API | üîÑ Phase 6.6 | Container orchestration |
| **Docker** | Generic + Docker API | üîÑ Phase 6.6 | Container runtime |

### Implementation Phases

#### **Phase 6.6.1: Generic Collector Core** (2 days)
- Create `generic_collector.py` base class
- Implement Prometheus scraping (universal)
- Implement DCGM GPU metrics (universal)
- Metric normalization and validation
- **Deliverable:** Universal collector works with Prometheus-only mode

#### **Phase 6.6.2: Provider API Integration** (3 days)
- Create provider API adapter interface
- Implement 12 provider API adapters:
  - `vultr_api.py`, `runpod_api.py`, `digitalocean_api.py`
  - `linode_api.py`, `hetzner_api.py`, `ovh_api.py`
  - `lambda_api.py`, `coreweave_api.py`, `paperspace_api.py`
  - `kubernetes_api.py`, `docker_api.py`
- Graceful fallback to Prometheus-only mode
- **Deliverable:** Billing/cost data collection from provider APIs

#### **Phase 6.6.3: Configuration Management** (1 day)
- Create `providers.yaml` configuration schema
- Add environment variables for all providers
- Configuration validation on startup
- Provider enable/disable flags
- **Deliverable:** Easy provider configuration

#### **Phase 6.6.4: Workflow Integration** (2 days)
- Update collection trigger API
- Integrate with Celery tasks
- Update Orchestrator routing
- Multi-provider collection support
- **Deliverable:** End-to-end collection workflow

#### **Phase 6.6.5: ClickHouse Schema** (1 day)
- Verify schema compatibility with all providers
- Create provider metadata table
- Migration scripts for new provider names
- Optimize queries for multi-provider
- **Deliverable:** Database ready for all providers

#### **Phase 6.6.6: Testing & Validation** (2 days)
- Unit tests for Generic Collector
- Integration tests for each provider
- Load testing with 10+ providers
- Documentation (architecture + how-to guides)
- **Deliverable:** Production-ready with 90%+ test coverage

#### **Phase 6.6.7: Portal UI for Multi-Cloud** (4 days)
- Cloud Provider Settings page
- Add Provider modal with provider grid
- Provider configuration forms (per provider)
- Connection status indicators
- Multi-cloud dashboard overview
- Provider management (add/edit/delete/test)
- **Deliverable:** Complete UI for managing all cloud providers

### Key Benefits

**1. Scalability**
- Add new providers with minimal code (just configuration)
- One Generic Collector handles 50+ providers
- No code duplication across providers

**2. Maintainability**
- Single codebase for all non-Big-3 providers
- Consistent behavior across providers
- Easy to debug and update

**3. Flexibility**
- Works with Prometheus-only (no API required)
- Optional API integration for billing data
- Supports on-premises and edge deployments

**4. Customer Experience**
- Support any infrastructure customer uses
- No vendor lock-in
- Unified view across all clouds

### Technical Implementation

**Generic Collector Pattern:**
```python
class GenericCollector(BaseCollector):
    """
    Universal collector for any cloud provider
    
    Works with:
    - Prometheus (required): Performance, GPU, application metrics
    - DCGM (optional): Detailed GPU metrics
    - Provider API (optional): Billing, cost, instance info
    """
    
    async def collect_all_metrics(self, config):
        # 1. Prometheus (universal)
        perf_metrics = await self.scrape_prometheus(config["prometheus_url"])
        
        # 2. DCGM (if GPU workload)
        gpu_metrics = await self.scrape_dcgm(config["dcgm_url"]) if config.get("dcgm_url") else {}
        
        # 3. Provider API (if available)
        cost_metrics = await self.call_provider_api(config) if config.get("api_key") else {}
        
        return self.normalize_metrics({
            **perf_metrics,
            **gpu_metrics,
            **cost_metrics
        })
```

**Provider API Adapter Pattern:**
```python
# Each provider has a simple adapter (50-100 lines)
class VultrAPIAdapter:
    async def get_billing_info(self, api_key):
        # Vultr-specific API call
        pass
    
    async def get_instance_info(self, api_key, instance_id):
        # Vultr-specific API call
        pass

# Easy to add new providers
class NewProviderAPIAdapter:
    async def get_billing_info(self, api_key):
        # New provider API call
        pass
```

### Portal UI Components

**1. Cloud Provider Settings Page**
- List of connected providers with status
- Add new provider button
- Configure/disconnect actions per provider
- Last sync timestamp and health status

**2. Provider Selector Modal**
- Grid of all supported providers with logos
- Search and filter capabilities
- Categorized by type (Big 3, GPU Cloud, General, Self-Hosted)
- One-click provider selection

**3. Provider Configuration Forms**
- Dynamic forms based on provider type
- API key/credentials input
- Endpoint configuration (Prometheus, DCGM, API)
- Collection settings (frequency, data types)
- Connection testing before save
- Real-time validation

**4. Multi-Cloud Dashboard**
- Aggregated view across all providers
- Cost breakdown by provider
- Performance comparison
- Provider-specific recommendations
- Quick access to provider settings

### Success Metrics

After Phase 6.6 completion:
- ‚úÖ Support 15+ cloud providers (3 dedicated + 12+ generic)
- ‚úÖ Single Generic Collector handles unlimited providers
- ‚úÖ Add new provider in <1 hour (just configuration)
- ‚úÖ Prometheus-only mode works for any infrastructure
- ‚úÖ Optional API integration for billing data
- ‚úÖ Complete Portal UI for provider management
- ‚úÖ 90%+ test coverage
- ‚úÖ Production-ready documentation

### Files Created/Modified

**New Files (51 total):**
- `services/data-collector/src/collectors/generic_collector.py`
- `services/data-collector/src/collectors/providers/*.py` (12 adapters)
- `services/data-collector/config/providers.yaml`
- `services/portal/src/pages/settings/cloud-providers.tsx`
- `services/portal/src/components/providers/*.tsx` (5 components)
- `services/portal/public/logos/*.svg` (12 logos)
- Test files, documentation, migrations

**Modified Files:**
- `services/data-collector/src/config.py`
- `services/data-collector/src/main.py`
- `services/data-collector/.env.example`
- `docker-compose.yml`
- ClickHouse schemas

### Timeline

| Phase | Duration | Deliverable |
|-------|----------|-------------|
| 6.6.1 | 2 days | Generic Collector Core |
| 6.6.2 | 3 days | Provider API Integration |
| 6.6.3 | 1 day | Configuration Management |
| 6.6.4 | 2 days | Workflow Integration |
| 6.6.5 | 1 day | ClickHouse Schema Updates |
| 6.6.6 | 2 days | Testing & Validation |
| 6.6.7 | 4 days | Portal UI |
| **Total** | **15 days** | **Multi-cloud support complete** |

---

## üéä Summary

**OptiInfra Architecture = 6 Layers:**

1. **Collection Layer:** Gather metrics from multi-cloud
2. **Storage Layer:** Store efficiently (PostgreSQL + ClickHouse + Qdrant + Redis)
3. **Orchestration Layer:** Coordinate agents intelligently
4. **Agent Layer:** AI-powered optimization (4 agents + Groq LLM)
5. **API Layer:** Fast access (V1 + V2)
6. **Portal Layer:** Beautiful UI for control

**Data Flow:** Customer ‚Üí Collector ‚Üí Storage ‚Üí Agent ‚Üí API ‚Üí Portal

**Key Innovation:** Multi-agent AI coordination with LLM-powered insights

**Result:** 50% cost savings + 3x performance + quality assurance

---

**Architecture Status:** üîÑ Phase 6.6 In Progress - Multi-Cloud Support

**Current Phase:** Phase 6.6 - Generic Collector Implementation (15 days)

**Next:** Phase 6.6.1 - Generic Collector Core
